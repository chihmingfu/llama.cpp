# TinyLlama FFN Norm Fake Quantization æŠ€è¡“ç¢ºèªåˆ†æ

## ğŸ¯ å•é¡Œç¢ºèª

**ç”¨æˆ¶å•é¡Œ**: ç¢ºèª TinyLlama ä¸­ `blk.21.ffn_norm.weight` æ˜¯å¦æœƒå—åˆ°å¯¦é©—è¨­è¨ˆä¸­çš„ fake quantization å½±éŸ¿ï¼Œä¸¦æä¾›éœ€è¦ä¿®æ”¹çš„å…·é«”å‡½æ•¸ã€‚

## âœ… é—œéµç™¼ç¾ï¼šç¢ºèªæœƒå—åˆ°å½±éŸ¿

### 1. TinyLlama æ¶æ§‹ç¢ºèª
- **æ¨¡å‹æ¶æ§‹**: `LLM_ARCH_LLAMA` (æ¨™æº– LLaMA æ¶æ§‹)
- **å±¤æ•¸**: 22å±¤ (0-21)ï¼Œæ‰€ä»¥ `blk.21` æ˜¯æœ€å¾Œä¸€å±¤
- **FFN Normé¡å‹**: `LLM_NORM_RMS` (RMS Normalization)

### 2. å…·é«”èª¿ç”¨è·¯å¾‘åˆ†æ

#### 2.1 FFN Norm æ¬Šé‡å‰µå»º
**ä½ç½®**: `src/llama-model.cpp:1898`
```cpp
layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, "weight", i), {n_embd}, 0);
```
- TinyLlamaçš„ `blk.21.ffn_norm.weight` åœ¨æ­¤è™•å‰µå»º
- ç¶­åº¦: `{2048}` (n_embd = 2048)

#### 2.2 å‰å‘å‚³æ’­èª¿ç”¨è·¯å¾‘
**ä½ç½®**: `src/llama-model.cpp:5080-5083` (llm_build_llamaå‡½æ•¸)
```cpp
cur = build_norm(ffn_inp,
        model.layers[il].ffn_norm, NULL,
        LLM_NORM_RMS, il);
cb(cur, "ffn_norm", il);
```

#### 2.3 build_norm å‡½æ•¸å¯¦ç¾
**ä½ç½®**: `src/llama-graph.cpp:475-484`
```cpp
ggml_tensor * llm_graph_context::build_norm(
         ggml_tensor * cur,
         ggml_tensor * mw,
         ggml_tensor * mb,
       llm_norm_type   type,
                 int   il) const {
    switch (type) {
        case LLM_NORM_RMS:   cur = ggml_rms_norm(ctx0, cur, hparams.f_norm_rms_eps); break;
        // ...
    }
    
    if (mw) {
        cur = ggml_mul(ctx0, cur, mw);  // é€™è£¡ä½¿ç”¨ ffn_norm weight
    }
    return cur;
}
```

### 3. Fake Quantization æ‡‰ç”¨é»ç¢ºèª

#### 3.1 ç•¶å‰èª¿ç”¨é»
```cpp
// src/llama-model.cpp:5083
cb(cur, "ffn_norm", il);  // il=21 æ™‚ï¼Œé€™è£¡æœƒè¢«èª¿ç”¨
```

#### 3.2 éœ€è¦æ·»åŠ çš„ Fake Quantization ä»£ç¢¼
**åœ¨ `cb(cur, "ffn_norm", il);` ä¹‹å¾Œç«‹å³æ·»åŠ :**
```cpp
// åœ¨æ¯å€‹ cb(cur, "ffn_norm", il) å¾Œæ·»åŠ 
if (cparams.fake_quant_ffn_norm_enabled && 
    (cparams.fake_quant_target_layer == -1 || cparams.fake_quant_target_layer == il)) {
    
    // ç¢ºä¿ tensor æ˜¯ F32 é¡å‹ä¸”æœ‰æ•¸æ“š
    if (cur->type == GGML_TYPE_F32 && cur->data != nullptr) {
        float* data = (float*)cur->data;
        size_t n_elements = ggml_nelements(cur);
        
        // æ‡‰ç”¨å‡é‡åŒ–
        llama_fake_quantize_data(data, n_elements, cparams.fake_quant_type);
        
        // Debug è¼¸å‡º
        if (cparams.fake_quant_compare) {
            LLAMA_LOG_INFO("Applied fake quantization to ffn_norm layer %d\n", il);
        }
    }
}
```

## ğŸ”§ éœ€è¦ä¿®æ”¹çš„å…·é«”å‡½æ•¸å’Œä½ç½®

### 1. åƒæ•¸çµæ§‹é«”æ“´å±•
**æ–‡ä»¶**: `src/llama-cparams.h`
```cpp
struct llama_cparams {
    // ç¾æœ‰åƒæ•¸...
    
    // æ–°å¢ FFN norm fake quantization åƒæ•¸
    bool fake_quant_ffn_norm_enabled = false;  // å•Ÿç”¨FFN normå‡é‡åŒ–
    int  fake_quant_target_layer = -1;         // ç›®æ¨™å±¤è™Ÿ (-1=æ‰€æœ‰å±¤)
    float fake_quant_ffn_scale = 1.0f;         // FFN normé‡åŒ–æ¯”ä¾‹
};
```

### 2. å‘½ä»¤è¡Œåƒæ•¸è§£æ
**æ–‡ä»¶**: `common/arg.cpp`
```cpp
// æ·»åŠ æ–°çš„å‘½ä»¤è¡Œåƒæ•¸
{"fake-quant-ffn-norm",  required_argument, 0, 0},
{"fake-quant-layer",     required_argument, 0, 0},
{"fake-quant-ffn-scale", required_argument, 0, 0},
```

### 3. APIæ¥å£æ“´å±•
**æ–‡ä»¶**: `include/llama.h`
```cpp
struct llama_context_params {
    // ç¾æœ‰åƒæ•¸...
    bool  fake_quant_ffn_norm_enabled;
    int   fake_quant_target_layer;
    float fake_quant_ffn_scale;
};
```

### 4. æ ¸å¿ƒä¿®æ”¹ä½ç½®åˆ—è¡¨

#### éœ€è¦åœ¨æ‰€æœ‰ä»¥ä¸‹ä½ç½®æ·»åŠ  fake quantization ä»£ç¢¼:
```cpp
// LLAMA æ¶æ§‹ (TinyLlama ä½¿ç”¨æ­¤æ¶æ§‹)
src/llama-model.cpp:5083   // éMoEç‰ˆæœ¬
src/llama-model.cpp:5097   // MoEç‰ˆæœ¬

// å…¶ä»–æ¶æ§‹ï¼ˆç‚ºäº†å®Œæ•´æ”¯æŒï¼‰
src/llama-model.cpp:5256   // LLAMAè®Šç¨®
src/llama-model.cpp:6197   // BAICHUAN
src/llama-model.cpp:6294   // BAICHUANè®Šç¨®
// ... ç´„50+å€‹å…¶ä»–æ¶æ§‹ä½ç½®
```

### 5. é—œéµä¿®æ”¹æ¨¡æ¿
```cpp
// åœ¨æ¯å€‹ cb(cur, "ffn_norm", il); ä¹‹å¾Œæ·»åŠ 
if (cparams.fake_quant_ffn_norm_enabled && 
    (cparams.fake_quant_target_layer == -1 || cparams.fake_quant_target_layer == il)) {
    
    if (cur->type == GGML_TYPE_F32 && cur->data != nullptr) {
        float* data = (float*)cur->data;
        size_t n_elements = ggml_nelements(cur);
        llama_fake_quantize_data(data, n_elements, cparams.fake_quant_type);
        
        if (cparams.fake_quant_compare) {
            LLAMA_LOG_INFO("FFN norm fake quant applied: layer %d, elements %zu\n", il, n_elements);
        }
    }
}
```

## âœ… ç¢ºèªçµæœ

### 1. **æœƒå—åˆ°å½±éŸ¿**: âœ… ç¢ºèª
- TinyLlama çš„ `blk.21.ffn_norm.weight` æœƒåœ¨ `llm_build_llama` å‡½æ•¸ä¸­è¢«è™•ç†
- è©²æ¬Šé‡æœƒé€šé `build_norm` â†’ `ggml_rms_norm` â†’ `ggml_mul` çš„è·¯å¾‘å½±éŸ¿è¨ˆç®—
- åœ¨ `cb(cur, "ffn_norm", 21)` èª¿ç”¨å¾Œï¼Œ`cur` tensor åŒ…å«äº†ç¶“é ffn_norm è™•ç†çš„çµæœ

### 2. **æ•¸æ“šæµç¢ºèª**: âœ… æ­£ç¢º
```
ffn_inp â†’ build_norm(ffn_inp, ffn_norm_weight) â†’ cur â†’ cb(cur, "ffn_norm", 21) â†’ [æˆ‘å€‘çš„fake quant]
```

### 3. **å½±éŸ¿ç¯„åœ**: âœ… ç¬¦åˆé æœŸ
- **ç›´æ¥å½±éŸ¿**: FFN å±¤çš„è¼¸å…¥æ­¸ä¸€åŒ–çµæœ
- **ç´¯ç©å½±éŸ¿**: å½±éŸ¿å¾ŒçºŒ FFN å±¤è¨ˆç®—å’Œæœ€çµ‚è¼¸å‡º
- **æ•æ„Ÿæ€§**: æ¯” logits å‡é‡åŒ–æ›´æ—©ä»‹å…¥ï¼Œå½±éŸ¿æ›´å¤§

### 4. **æŠ€è¡“å¯è¡Œæ€§**: âœ… å®Œå…¨å¯è¡Œ
- ç¾æœ‰ `llama_fake_quantize_data` å‡½æ•¸å¯ç›´æ¥ä½¿ç”¨
- æ•¸æ“šé¡å‹ç‚º F32ï¼Œé©åˆå‡é‡åŒ–è™•ç†
- æ•¸æ“šå·²åœ¨å…§å­˜ä¸­ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹

## ğŸ§ª æ¸¬è©¦é©—è­‰æ–¹æ³•

### æ¸¬è©¦å‘½ä»¤ç¤ºä¾‹:
```bash
# åŸºæº–æ¸¬è©¦ (ç„¡å‡é‡åŒ–)
./build/bin/llama-cli -m tinyllama.gguf -p "æ¸¬è©¦" -n 10 --seed 42

# æ¸¬è©¦ layer 21 çš„ FFN norm BF16 å‡é‡åŒ–
./build/bin/llama-cli -m tinyllama.gguf --fake-quant-ffn-norm bf16 \
  --fake-quant-layer 21 -p "æ¸¬è©¦" -n 10 --seed 42

# æ¸¬è©¦æ‰€æœ‰å±¤çš„ FFN norm BF16 å‡é‡åŒ–  
./build/bin/llama-cli -m tinyllama.gguf --fake-quant-ffn-norm bf16 \
  -p "æ¸¬è©¦" -n 10 --seed 42
```

### é æœŸçµæœ:
- èƒ½çœ‹åˆ° `"FFN norm fake quant applied: layer 21"` çš„æ—¥å¿—è¼¸å‡º
- èˆ‡åŸºæº–ç›¸æ¯”æœƒæœ‰æ˜é¡¯çš„è¼¸å‡ºå·®ç•°ï¼ˆæ¯” logits å‡é‡åŒ–å½±éŸ¿æ›´å¤§ï¼‰
- layer 21 å–®ç¨æ¸¬è©¦å½±éŸ¿ç›¸å°è¼ƒå°ï¼Œå…¨å±¤æ¸¬è©¦å½±éŸ¿é¡¯è‘—

## ğŸ“Š çµè«–

**âœ… ç¢ºèª**: TinyLlama çš„ `blk.21.ffn_norm.weight` **æœƒè¢«å¯¦é©—è¨­è¨ˆçœŸå¯¦å½±éŸ¿**

1. **è·¯å¾‘ç¢ºèª**: æ¬Šé‡æœƒåœ¨å‰å‘å‚³æ’­ä¸­è¢«ä½¿ç”¨ï¼Œè¨ˆç®—çµæœæœƒè¢«å‡é‡åŒ–è™•ç†
2. **ä½ç½®ç¢ºèª**: `src/llama-model.cpp:5083` æ˜¯é—œéµä¿®æ”¹é»
3. **å½±éŸ¿ç¢ºèª**: å‡é‡åŒ–æœƒå½±éŸ¿ FFN å±¤çš„æ­¸ä¸€åŒ–è¼¸å‡ºï¼Œé€²è€Œå½±éŸ¿æ•´å€‹æ¨ç†çµæœ
4. **å¯¦ç¾ç¢ºèª**: æŠ€è¡“å¯¦ç¾å®Œå…¨å¯è¡Œï¼Œä¿®æ”¹é»æ˜ç¢º

**å»ºè­°**: å¯ä»¥é–‹å§‹å¯¦æ–½ï¼Œé æœŸæœƒè§€å¯Ÿåˆ°æ¯”ç¾æœ‰ logits å‡é‡åŒ–æ›´æ˜é¡¯çš„å½±éŸ¿ã€‚