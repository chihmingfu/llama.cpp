# Fake Quantization with BF16 Support in llama.cpp

## ğŸ“‹ ç ”ç©¶ç¸½çµèˆ‡å¯¦ç¾è¨ˆåŠƒ

### ğŸ¯ å°ˆæ¡ˆç›®æ¨™
å¯¦ç¾ fake quantization æ©Ÿåˆ¶ï¼Œä½¿ç”¨ BF16 (Brain Float 16) å–ä»£ FP16 æˆ– FP32ï¼Œä»¥æ¸¬è©¦é‡åŒ–å°æ¨¡å‹ç²¾åº¦çš„å½±éŸ¿ï¼ŒåŒæ™‚ä¿æŒè¨ˆç®—æ•ˆç‡ã€‚

---

## ğŸ” ç¾ç‹€åˆ†æ

### âœ… BF16 æ”¯æŒç¾ç‹€
ç¶“éæ·±å…¥ä»£ç¢¼åˆ†æï¼Œç™¼ç¾ **GGML å·²ç¶“å…·å‚™å®Œæ•´çš„ BF16 æ”¯æ´**ï¼š

#### æ•¸æ“šé¡å‹å®šç¾©
```cpp
// ggml/include/ggml.h:389
enum ggml_type {
    GGML_TYPE_F32     = 0,
    GGML_TYPE_F16     = 1,
    // ... å…¶ä»–é‡åŒ–é¡å‹
    GGML_TYPE_BF16    = 30,  // âœ… å·²å®šç¾©
    GGML_TYPE_COUNT   = 39,
};

// ggml/include/ggml.h:432
enum ggml_ftype {
    GGML_FTYPE_MOSTLY_BF16 = 24, // âœ… æª”æ¡ˆæ ¼å¼æ”¯æ´
};
```

#### è½‰æ›å‡½æ•¸
```cpp
// ggml/include/ggml.h:346-351
typedef struct { uint16_t bits; } ggml_bf16_t;
GGML_API ggml_bf16_t ggml_fp32_to_bf16(float);
GGML_API float       ggml_bf16_to_fp32(ggml_bf16_t);
GGML_API void        ggml_bf16_to_fp32_row(const ggml_bf16_t *, float *, int64_t);
GGML_API void        ggml_fp32_to_bf16_row(const float *, ggml_bf16_t *, int64_t);
```

#### ç¡¬é«”åŠ é€Ÿæ”¯æ´
```cpp
// CMakeLists.txt ä¸­çš„é¸é …
option(GGML_AVX512_BF16 "ggml: enable AVX512-BF16" OFF)
option(GGML_AMX_BF16    "ggml: enable AMX-BF16"    OFF)
```

#### å‘é‡åŒ–æ“ä½œ
```cpp
// ggml/src/ggml-cpu/vec.h:43
void ggml_vec_dot_bf16(int n, float * GGML_RESTRICT s, 
                       size_t bs, ggml_bf16_t * GGML_RESTRICT x, 
                       size_t bx, ggml_bf16_t * GGML_RESTRICT y, 
                       size_t by, int nrc);
```

### ğŸ—ï¸ ç¾æœ‰é‡åŒ–æ¶æ§‹
GGML æ”¯æ´å¤šç¨®é‡åŒ–æ ¼å¼ï¼š
- **æµ®é»æ•¸é¡å‹**: F32, F16, BF16, F64
- **æ•´æ•¸é‡åŒ–**: Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, Q8_1
- **K-é‡åŒ–**: Q2_K, Q3_K, Q4_K, Q5_K, Q6_K, Q8_K
- **IQ ç³»åˆ—**: IQ1_S, IQ2_XXS, IQ2_XS, IQ3_XXS, IQ3_S, IQ4_NL, IQ4_XS
- **æ–°å‹é‡åŒ–**: TQ1_0, TQ2_0

---

## ğŸ’¡ BF16 vs FP16 æ¯”è¼ƒ

| ç‰¹æ€§ | FP32 | FP16 | BF16 |
|------|------|------|------|
| **ç¸½ä½æ•¸** | 32 | 16 | 16 |
| **ç¬¦è™Ÿä½** | 1 | 1 | 1 |
| **æŒ‡æ•¸ä½** | 8 | 5 | 8 |
| **å°¾æ•¸ä½** | 23 | 10 | 7 |
| **æ•¸å€¼ç¯„åœ** | Â±3.4Ã—10Â³â¸ | Â±6.5Ã—10â´ | Â±3.4Ã—10Â³â¸ |
| **ç²¾åº¦** | ~7ä½åé€²ä½ | ~3ä½åé€²ä½ | ~2ä½åé€²ä½ |

### ğŸ¯ BF16 å„ªå‹¢
1. **ç›¸åŒæŒ‡æ•¸ç¯„åœ**: èˆ‡ FP32 ç›¸åŒçš„ 8-bit æŒ‡æ•¸ï¼Œé¿å…æº¢å‡ºå•é¡Œ
2. **æ›´å¥½çš„æ•¸å€¼ç©©å®šæ€§**: è¼ƒå°‘çš„æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å•é¡Œ
3. **ç¡¬é«”æ”¯æ´**: Intelã€AMDã€NVIDIA éƒ½æœ‰å°ˆé–€çš„ BF16 æŒ‡ä»¤é›†
4. **è¨˜æ†¶é«”æ•ˆç‡**: æ¯” FP32 ç¯€çœ 50% è¨˜æ†¶é«”

---

## ğŸš€ Fake Quantization å¯¦ç¾æ–¹æ¡ˆ

### æ ¸å¿ƒæ¦‚å¿µ
**Fake Quantization** æ˜¯æŒ‡åœ¨æ¨ç†éç¨‹ä¸­æ¨¡æ“¬é‡åŒ–çš„ç²¾åº¦æå¤±ï¼Œä½†ä¸å¯¦éš›æ”¹è®Šæ¨¡å‹å„²å­˜æ ¼å¼ï¼š

1. **æ¨¡å‹å„²å­˜**: ä¿æŒåŸå§‹ç²¾åº¦ (FP32/FP16)
2. **æ¨ç†è¨ˆç®—**: å‹•æ…‹è½‰æ›ç‚ºç›®æ¨™ç²¾åº¦ (BF16)
3. **ç²¾åº¦æå¤±**: çœŸå¯¦åæ˜ é‡åŒ–å¾Œçš„æ¨¡å‹è¡¨ç¾
4. **éˆæ´»æ€§**: å¯éš¨æ™‚åˆ‡æ›ä¸åŒç²¾åº¦æ¨¡å¼

### ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ

#### 1. åƒæ•¸é…ç½®
```cpp
// common/common.h - æ–°å¢åƒæ•¸
struct llama_context_params {
    // ... ç¾æœ‰åƒæ•¸
    bool fake_quant_enabled;           // å•Ÿç”¨ fake quantization
    enum ggml_type fake_quant_type;    // ç›®æ¨™é‡åŒ–é¡å‹ (å¦‚ GGML_TYPE_BF16)
    float fake_quant_scale;            // é‡åŒ–æ¯”ä¾‹ (0.0-1.0, 1.0=å…¨éƒ¨é‡åŒ–)
};
```

#### 2. å‹•æ…‹é‡åŒ–å±¤
```cpp
// æ–°æª”æ¡ˆ: src/llama-fake-quant.h
#pragma once

#include "ggml.h"

// Fake quantization å‡½æ•¸
void llama_fake_quantize_tensor(
    struct ggml_tensor * tensor,
    enum ggml_type target_type,
    float scale_factor
);

// æ‰¹æ¬¡è™•ç†
void llama_fake_quantize_model_tensors(
    struct llama_model * model,
    const struct llama_fake_quant_params * params
);
```

#### 3. æ¨ç†ç®¡é“æ•´åˆ
```cpp
// src/llama-fake-quant.cpp
static void apply_fake_quantization_forward(
    const struct ggml_tensor * src,
    struct ggml_tensor * dst,
    enum ggml_type fake_type
) {
    switch (fake_type) {
        case GGML_TYPE_BF16: {
            // å‹•æ…‹è½‰æ›: FP32 -> BF16 -> FP32
            const float * src_data = (const float *)src->data;
            float * dst_data = (float *)dst->data;
            
            for (int64_t i = 0; i < ggml_nelements(src); i++) {
                // æ¨¡æ“¬ BF16 ç²¾åº¦æå¤±
                ggml_bf16_t bf16_val = ggml_compute_fp32_to_bf16(src_data[i]);
                dst_data[i] = ggml_compute_bf16_to_fp32(bf16_val);
            }
            break;
        }
        // æ”¯æ´å…¶ä»–é‡åŒ–é¡å‹...
    }
}
```

### ğŸ“ æª”æ¡ˆçµæ§‹
```
src/
â”œâ”€â”€ llama-fake-quant.h          # Fake quantization API
â”œâ”€â”€ llama-fake-quant.cpp        # æ ¸å¿ƒå¯¦ç¾
â”œâ”€â”€ llama-context.cpp           # æ•´åˆåˆ°æ¨ç†ç®¡é“
â””â”€â”€ llama-model.cpp            # æ¨¡å‹è¼‰å…¥æ™‚çš„æ”¯æ´

tools/
â”œâ”€â”€ main/main.cpp              # å‘½ä»¤è¡Œåƒæ•¸æ”¯æ´
â””â”€â”€ server/server.cpp          # ä¼ºæœå™¨ API æ”¯æ´

common/
â””â”€â”€ common.cpp                 # åƒæ•¸è§£æ
```

---

## ğŸ› ï¸ å¯¦æ–½è¨ˆåŠƒ

### Phase 1: åŸºç¤å¯¦ç¾ (1-2 é€±)
- [ ] æ·»åŠ  fake quantization å‘½ä»¤è¡Œåƒæ•¸
- [ ] å¯¦ç¾åŸºæœ¬çš„ BF16 fake quantization
- [ ] æ•´åˆåˆ°ä¸»è¦æ¨ç†å¾ªç’°
- [ ] åŸºæœ¬åŠŸèƒ½æ¸¬è©¦

#### å‘½ä»¤è¡Œä»‹é¢
```bash
# å•Ÿç”¨ BF16 fake quantization
./llama-cli -m model.gguf --fake-quant bf16 -p "Hello world"

# éƒ¨åˆ†é‡åŒ– (50% çš„å±¤ä½¿ç”¨ BF16)
./llama-cli -m model.gguf --fake-quant bf16 --fake-quant-scale 0.5

# å°æ¯”æ¨¡å¼ (åŒæ™‚è¼¸å‡ºåŸå§‹å’Œé‡åŒ–çµæœ)
./llama-cli -m model.gguf --fake-quant bf16 --compare-precision
```

### Phase 2: å„ªåŒ–èˆ‡æ“´å±• (2-3 é€±)
- [ ] å‘é‡åŒ– fake quantization æ“ä½œ
- [ ] æ”¯æ´æ··åˆç²¾åº¦ (ä¸åŒå±¤ä½¿ç”¨ä¸åŒç²¾åº¦)
- [ ] æ·»åŠ å…¶ä»–é‡åŒ–é¡å‹æ”¯æ´ (Q4_0, Q8_0 ç­‰)
- [ ] æ€§èƒ½å„ªåŒ–

#### æ··åˆç²¾åº¦é…ç½®
```cpp
// é…ç½®æª”æ¡ˆ: fake_quant_config.json
{
    "layers": {
        "attention": "bf16",
        "feed_forward": "q4_0", 
        "embedding": "fp16",
        "output": "fp32"
    },
    "scale_factor": 1.0
}
```

### Phase 3: è©•ä¼°èˆ‡æ¸¬è©¦ (1-2 é€±)
- [ ] ç²¾åº¦æå¤±åˆ†æå·¥å…·
- [ ] æ€§èƒ½åŸºæº–æ¸¬è©¦
- [ ] è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ
- [ ] èˆ‡çœŸå¯¦é‡åŒ–æ¨¡å‹å°æ¯”

#### è©•ä¼°å·¥å…·
```bash
# ç²¾åº¦åˆ†æ
./llama-bench --model model.gguf --fake-quant bf16 --precision-analysis

# æ€§èƒ½æ¸¬è©¦
./llama-bench --model model.gguf --fake-quant bf16 --performance-test

# è¨˜æ†¶é«”åˆ†æ
./llama-cli --model model.gguf --fake-quant bf16 --memory-profile
```

---

## ğŸ“Š é æœŸæ•ˆæœ

### ç²¾åº¦å½±éŸ¿
- **BF16**: é æœŸç²¾åº¦æå¤± < 5%
- **è¨˜æ†¶é«”ç¯€çœ**: ç†è«–ä¸Š 0% (fake quantization ä¸æ”¹è®Šå„²å­˜)
- **è¨ˆç®—æ•ˆç‡**: åœ¨æ”¯æ´ BF16 çš„ç¡¬é«”ä¸Šæå‡ 10-30%

### æ¸¬è©¦æŒ‡æ¨™
1. **Perplexity æ¸¬è©¦**: ä½¿ç”¨ WikiText-2 dataset
2. **æ¨ç†é€Ÿåº¦**: tokens/second æ¯”è¼ƒ
3. **è¨˜æ†¶é«”ä½¿ç”¨**: å³°å€¼è¨˜æ†¶é«”åˆ†æ
4. **ç²¾åº¦ä¿æŒ**: èˆ‡åŸæ¨¡å‹çš„è¼¸å‡ºå·®ç•°

---

## ğŸ”§ æŠ€è¡“ç´°ç¯€

### BF16 è½‰æ›å¯¦ç¾
```cpp
// å·²å­˜åœ¨æ–¼ ggml-impl.h
static inline ggml_bf16_t ggml_compute_fp32_to_bf16(float s) {
    ggml_bf16_t h;
    union { float f; uint32_t i; } u;
    u.f = s;
    if ((u.i & 0x7fffffff) > 0x7f800000) { /* nan */
        h.bits = (u.i >> 16) | 64; /* force to quiet */
        return h;
    }
    h.bits = (u.i + (0x7fff + ((u.i >> 16) & 1))) >> 16;
    return h;
}

static inline float ggml_compute_bf16_to_fp32(ggml_bf16_t h) {
    union { float f; uint32_t i; } u;
    u.i = (uint32_t)h.bits << 16;
    return u.f;
}
```

### å‘é‡åŒ–å„ªåŒ–
```cpp
// ä½¿ç”¨ AVX512BF16 æŒ‡ä»¤é›†
#if defined(__AVX512BF16__)
void fake_quant_fp32_to_bf16_avx512(const float* src, float* dst, int n) {
    for (int i = 0; i + 16 <= n; i += 16) {
        __m512 fp32_vec = _mm512_loadu_ps(src + i);
        __m256i bf16_vec = _mm512_cvtneps_pbh(fp32_vec);
        __m512 result = _mm512_cvtpbh_ps(bf16_vec);
        _mm512_storeu_ps(dst + i, result);
    }
}
#endif
```

---

## ğŸš§ å·²çŸ¥é™åˆ¶èˆ‡æŒ‘æˆ°

### æŠ€è¡“æŒ‘æˆ°
1. **æ€§èƒ½é–‹éŠ·**: å‹•æ…‹è½‰æ›å¯èƒ½å½±éŸ¿æ¨ç†é€Ÿåº¦
2. **è¨˜æ†¶é«”æ‹·è²**: éœ€è¦é¡å¤–çš„ç·©è¡å€é€²è¡Œè½‰æ›
3. **ç²¾åº¦æ§åˆ¶**: å¦‚ä½•ç²¾ç¢ºæ§åˆ¶é‡åŒ–çš„æ‡‰ç”¨ç¯„åœ

### è§£æ±ºæ–¹æ¡ˆ
1. **æƒ°æ€§é‡åŒ–**: åªåœ¨å¿…è¦æ™‚é€²è¡Œè½‰æ›
2. **In-place æ“ä½œ**: æ¸›å°‘è¨˜æ†¶é«”æ‹·è²
3. **å±¤ç´šæ§åˆ¶**: ç´°ç²’åº¦çš„é‡åŒ–æ§åˆ¶

---

## ğŸ“š ç›¸é—œè³‡æº

### ç¨‹å¼ç¢¼ä½ç½®
- **GGML BF16 å¯¦ç¾**: `ggml/src/ggml-impl.h:436-470`
- **æ•¸æ“šé¡å‹å®šç¾©**: `ggml/include/ggml.h:358-398`
- **è½‰æ›å‡½æ•¸**: `ggml/src/ggml.c:420-470`
- **å‘é‡æ“ä½œ**: `ggml/src/ggml-cpu/vec.h`

### åƒè€ƒæ–‡ç»
1. [Brain Floating Point Format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)
2. [Intel AVX512-BF16 Documentation](https://software.intel.com/content/www/us/en/develop/articles/intel-avx-512-bf16-instructions.html)
3. [GGML Documentation](https://github.com/ggml-org/ggml)

---

## âœ… ç¸½çµ

é€™å€‹ç ”ç©¶ç™¼ç¾ **GGML å·²ç¶“å…·å‚™å®Œæ•´çš„ BF16 åŸºç¤è¨­æ–½**ï¼Œé€™å¤§å¤§ç°¡åŒ–äº†å¯¦ç¾ fake quantization çš„è¤‡é›œåº¦ã€‚ä¸»è¦å·¥ä½œå°‡é›†ä¸­åœ¨ï¼š

1. **æ‡‰ç”¨å±¤æ•´åˆ**: å°‡ç¾æœ‰çš„ BF16 æ”¯æ´æ•´åˆåˆ°æ¨ç†ç®¡é“
2. **ç”¨æˆ¶ä»‹é¢**: æä¾›ç°¡å–®æ˜“ç”¨çš„å‘½ä»¤è¡Œå’Œ API ä»‹é¢  
3. **æ€§èƒ½å„ªåŒ–**: åˆ©ç”¨ç¾æœ‰çš„å‘é‡åŒ–å’Œç¡¬é«”åŠ é€ŸåŠŸèƒ½
4. **æ¸¬è©¦é©—è­‰**: ç¢ºä¿ç²¾åº¦å’Œæ€§èƒ½ç¬¦åˆé æœŸ

ç›¸æ¯”å¾é›¶é–‹å§‹å¯¦ç¾ BF16 æ”¯æ´ï¼Œé€™å€‹æ–¹æ¡ˆæ›´åŠ å¯è¡Œä¸”é¢¨éšªè¼ƒä½ã€‚