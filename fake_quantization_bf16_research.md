# Fake Quantization with BF16 Support in llama.cpp

## ğŸ“‹ ç ”ç©¶ç¸½çµèˆ‡å¯¦ç¾è¨ˆåŠƒ

### ğŸ¯ å°ˆæ¡ˆç›®æ¨™
å¯¦ç¾ fake quantization æ©Ÿåˆ¶ï¼Œä½¿ç”¨ BF16 (Brain Float 16) å–ä»£ FP16 æˆ– FP32ï¼Œä»¥æ¸¬è©¦é‡åŒ–å°æ¨¡å‹ç²¾åº¦çš„å½±éŸ¿ï¼ŒåŒæ™‚ä¿æŒè¨ˆç®—æ•ˆç‡ã€‚

### âœ… å¯¦ç¾ç‹€æ…‹ (2025-01-14 æ›´æ–°)
**é …ç›®å·²å®Œæˆä¸¦é€šéæ¸¬è©¦!** Fake quantization åŠŸèƒ½å·²æˆåŠŸé›†æˆåˆ° llama.cpp ä¸­ï¼Œæ”¯æ´ BF16ã€F16 å’Œ F32 å‡é‡åŒ–ã€‚

---

## ğŸ” ç¾ç‹€åˆ†æ

### âœ… BF16 æ”¯æŒç¾ç‹€
ç¶“éæ·±å…¥ä»£ç¢¼åˆ†æï¼Œç™¼ç¾ **GGML å·²ç¶“å…·å‚™å®Œæ•´çš„ BF16 æ”¯æ´**ï¼š

#### æ•¸æ“šé¡å‹å®šç¾©
```cpp
// ggml/include/ggml.h:389
enum ggml_type {
    GGML_TYPE_F32     = 0,
    GGML_TYPE_F16     = 1,
    // ... å…¶ä»–é‡åŒ–é¡å‹
    GGML_TYPE_BF16    = 30,  // âœ… å·²å®šç¾©
    GGML_TYPE_COUNT   = 39,
};

// ggml/include/ggml.h:432
enum ggml_ftype {
    GGML_FTYPE_MOSTLY_BF16 = 24, // âœ… æª”æ¡ˆæ ¼å¼æ”¯æ´
};
```

#### è½‰æ›å‡½æ•¸
```cpp
// ggml/include/ggml.h:346-351
typedef struct { uint16_t bits; } ggml_bf16_t;
GGML_API ggml_bf16_t ggml_fp32_to_bf16(float);
GGML_API float       ggml_bf16_to_fp32(ggml_bf16_t);
GGML_API void        ggml_bf16_to_fp32_row(const ggml_bf16_t *, float *, int64_t);
GGML_API void        ggml_fp32_to_bf16_row(const float *, ggml_bf16_t *, int64_t);
```

#### ç¡¬é«”åŠ é€Ÿæ”¯æ´
```cpp
// CMakeLists.txt ä¸­çš„é¸é …
option(GGML_AVX512_BF16 "ggml: enable AVX512-BF16" OFF)
option(GGML_AMX_BF16    "ggml: enable AMX-BF16"    OFF)
```

#### å‘é‡åŒ–æ“ä½œ
```cpp
// ggml/src/ggml-cpu/vec.h:43
void ggml_vec_dot_bf16(int n, float * GGML_RESTRICT s, 
                       size_t bs, ggml_bf16_t * GGML_RESTRICT x, 
                       size_t bx, ggml_bf16_t * GGML_RESTRICT y, 
                       size_t by, int nrc);
```

### ğŸ—ï¸ ç¾æœ‰é‡åŒ–æ¶æ§‹
GGML æ”¯æ´å¤šç¨®é‡åŒ–æ ¼å¼ï¼š
- **æµ®é»æ•¸é¡å‹**: F32, F16, BF16, F64
- **æ•´æ•¸é‡åŒ–**: Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, Q8_1
- **K-é‡åŒ–**: Q2_K, Q3_K, Q4_K, Q5_K, Q6_K, Q8_K
- **IQ ç³»åˆ—**: IQ1_S, IQ2_XXS, IQ2_XS, IQ3_XXS, IQ3_S, IQ4_NL, IQ4_XS
- **æ–°å‹é‡åŒ–**: TQ1_0, TQ2_0

---

## ğŸ’¡ BF16 vs FP16 æ¯”è¼ƒ

| ç‰¹æ€§ | FP32 | FP16 | BF16 |
|------|------|------|------|
| **ç¸½ä½æ•¸** | 32 | 16 | 16 |
| **ç¬¦è™Ÿä½** | 1 | 1 | 1 |
| **æŒ‡æ•¸ä½** | 8 | 5 | 8 |
| **å°¾æ•¸ä½** | 23 | 10 | 7 |
| **æ•¸å€¼ç¯„åœ** | Â±3.4Ã—10Â³â¸ | Â±6.5Ã—10â´ | Â±3.4Ã—10Â³â¸ |
| **ç²¾åº¦** | ~7ä½åé€²ä½ | ~3ä½åé€²ä½ | ~2ä½åé€²ä½ |

### ğŸ¯ BF16 å„ªå‹¢
1. **ç›¸åŒæŒ‡æ•¸ç¯„åœ**: èˆ‡ FP32 ç›¸åŒçš„ 8-bit æŒ‡æ•¸ï¼Œé¿å…æº¢å‡ºå•é¡Œ
2. **æ›´å¥½çš„æ•¸å€¼ç©©å®šæ€§**: è¼ƒå°‘çš„æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å•é¡Œ
3. **ç¡¬é«”æ”¯æ´**: Intelã€AMDã€NVIDIA éƒ½æœ‰å°ˆé–€çš„ BF16 æŒ‡ä»¤é›†
4. **è¨˜æ†¶é«”æ•ˆç‡**: æ¯” FP32 ç¯€çœ 50% è¨˜æ†¶é«”

---

## ğŸš€ Fake Quantization å¯¦ç¾æ–¹æ¡ˆ

### æ ¸å¿ƒæ¦‚å¿µ
**Fake Quantization** æ˜¯æŒ‡åœ¨æ¨ç†éç¨‹ä¸­æ¨¡æ“¬é‡åŒ–çš„ç²¾åº¦æå¤±ï¼Œä½†ä¸å¯¦éš›æ”¹è®Šæ¨¡å‹å„²å­˜æ ¼å¼ï¼š

1. **æ¨¡å‹å„²å­˜**: ä¿æŒåŸå§‹ç²¾åº¦ (FP32/FP16)
2. **æ¨ç†è¨ˆç®—**: å‹•æ…‹è½‰æ›ç‚ºç›®æ¨™ç²¾åº¦ (BF16)
3. **ç²¾åº¦æå¤±**: çœŸå¯¦åæ˜ é‡åŒ–å¾Œçš„æ¨¡å‹è¡¨ç¾
4. **éˆæ´»æ€§**: å¯éš¨æ™‚åˆ‡æ›ä¸åŒç²¾åº¦æ¨¡å¼

### ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ

#### 1. åƒæ•¸é…ç½®
```cpp
// common/common.h - æ–°å¢åƒæ•¸
struct llama_context_params {
    // ... ç¾æœ‰åƒæ•¸
    bool fake_quant_enabled;           // å•Ÿç”¨ fake quantization
    enum ggml_type fake_quant_type;    // ç›®æ¨™é‡åŒ–é¡å‹ (å¦‚ GGML_TYPE_BF16)
    float fake_quant_scale;            // é‡åŒ–æ¯”ä¾‹ (0.0-1.0, 1.0=å…¨éƒ¨é‡åŒ–)
};
```

#### 2. å‹•æ…‹é‡åŒ–å±¤
```cpp
// æ–°æª”æ¡ˆ: src/llama-fake-quant.h
#pragma once

#include "ggml.h"

// Fake quantization å‡½æ•¸
void llama_fake_quantize_tensor(
    struct ggml_tensor * tensor,
    enum ggml_type target_type,
    float scale_factor
);

// æ‰¹æ¬¡è™•ç†
void llama_fake_quantize_model_tensors(
    struct llama_model * model,
    const struct llama_fake_quant_params * params
);
```

#### 3. æ¨ç†ç®¡é“æ•´åˆ
```cpp
// src/llama-fake-quant.cpp
static void apply_fake_quantization_forward(
    const struct ggml_tensor * src,
    struct ggml_tensor * dst,
    enum ggml_type fake_type
) {
    switch (fake_type) {
        case GGML_TYPE_BF16: {
            // å‹•æ…‹è½‰æ›: FP32 -> BF16 -> FP32
            const float * src_data = (const float *)src->data;
            float * dst_data = (float *)dst->data;
            
            for (int64_t i = 0; i < ggml_nelements(src); i++) {
                // æ¨¡æ“¬ BF16 ç²¾åº¦æå¤±
                ggml_bf16_t bf16_val = ggml_compute_fp32_to_bf16(src_data[i]);
                dst_data[i] = ggml_compute_bf16_to_fp32(bf16_val);
            }
            break;
        }
        // æ”¯æ´å…¶ä»–é‡åŒ–é¡å‹...
    }
}
```

### ğŸ“ æª”æ¡ˆçµæ§‹
```
src/
â”œâ”€â”€ llama-fake-quant.h          # Fake quantization API
â”œâ”€â”€ llama-fake-quant.cpp        # æ ¸å¿ƒå¯¦ç¾
â”œâ”€â”€ llama-context.cpp           # æ•´åˆåˆ°æ¨ç†ç®¡é“
â””â”€â”€ llama-model.cpp            # æ¨¡å‹è¼‰å…¥æ™‚çš„æ”¯æ´

tools/
â”œâ”€â”€ main/main.cpp              # å‘½ä»¤è¡Œåƒæ•¸æ”¯æ´
â””â”€â”€ server/server.cpp          # ä¼ºæœå™¨ API æ”¯æ´

common/
â””â”€â”€ common.cpp                 # åƒæ•¸è§£æ
```

---

## ğŸ› ï¸ å¯¦æ–½è¨ˆåŠƒ

### Phase 1: åŸºç¤å¯¦ç¾ (1-2 é€±)
- [ ] æ·»åŠ  fake quantization å‘½ä»¤è¡Œåƒæ•¸
- [ ] å¯¦ç¾åŸºæœ¬çš„ BF16 fake quantization
- [ ] æ•´åˆåˆ°ä¸»è¦æ¨ç†å¾ªç’°
- [ ] åŸºæœ¬åŠŸèƒ½æ¸¬è©¦

#### å‘½ä»¤è¡Œä»‹é¢
```bash
# å•Ÿç”¨ BF16 fake quantization
./llama-cli -m model.gguf --fake-quant bf16 -p "Hello world"

# éƒ¨åˆ†é‡åŒ– (50% çš„å±¤ä½¿ç”¨ BF16)
./llama-cli -m model.gguf --fake-quant bf16 --fake-quant-scale 0.5

# å°æ¯”æ¨¡å¼ (åŒæ™‚è¼¸å‡ºåŸå§‹å’Œé‡åŒ–çµæœ)
./llama-cli -m model.gguf --fake-quant bf16 --compare-precision
```

### Phase 2: å„ªåŒ–èˆ‡æ“´å±• (2-3 é€±)
- [ ] å‘é‡åŒ– fake quantization æ“ä½œ
- [ ] æ”¯æ´æ··åˆç²¾åº¦ (ä¸åŒå±¤ä½¿ç”¨ä¸åŒç²¾åº¦)
- [ ] æ·»åŠ å…¶ä»–é‡åŒ–é¡å‹æ”¯æ´ (Q4_0, Q8_0 ç­‰)
- [ ] æ€§èƒ½å„ªåŒ–

#### æ··åˆç²¾åº¦é…ç½®
```cpp
// é…ç½®æª”æ¡ˆ: fake_quant_config.json
{
    "layers": {
        "attention": "bf16",
        "feed_forward": "q4_0", 
        "embedding": "fp16",
        "output": "fp32"
    },
    "scale_factor": 1.0
}
```

### Phase 3: è©•ä¼°èˆ‡æ¸¬è©¦ (1-2 é€±)
- [ ] ç²¾åº¦æå¤±åˆ†æå·¥å…·
- [ ] æ€§èƒ½åŸºæº–æ¸¬è©¦
- [ ] è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ
- [ ] èˆ‡çœŸå¯¦é‡åŒ–æ¨¡å‹å°æ¯”

#### è©•ä¼°å·¥å…·
```bash
# ç²¾åº¦åˆ†æ
./llama-bench --model model.gguf --fake-quant bf16 --precision-analysis

# æ€§èƒ½æ¸¬è©¦
./llama-bench --model model.gguf --fake-quant bf16 --performance-test

# è¨˜æ†¶é«”åˆ†æ
./llama-cli --model model.gguf --fake-quant bf16 --memory-profile
```

---

## ğŸ“Š é æœŸæ•ˆæœ

### ç²¾åº¦å½±éŸ¿
- **BF16**: é æœŸç²¾åº¦æå¤± < 5%
- **è¨˜æ†¶é«”ç¯€çœ**: ç†è«–ä¸Š 0% (fake quantization ä¸æ”¹è®Šå„²å­˜)
- **è¨ˆç®—æ•ˆç‡**: åœ¨æ”¯æ´ BF16 çš„ç¡¬é«”ä¸Šæå‡ 10-30%

### æ¸¬è©¦æŒ‡æ¨™
1. **Perplexity æ¸¬è©¦**: ä½¿ç”¨ WikiText-2 dataset
2. **æ¨ç†é€Ÿåº¦**: tokens/second æ¯”è¼ƒ
3. **è¨˜æ†¶é«”ä½¿ç”¨**: å³°å€¼è¨˜æ†¶é«”åˆ†æ
4. **ç²¾åº¦ä¿æŒ**: èˆ‡åŸæ¨¡å‹çš„è¼¸å‡ºå·®ç•°

---

## ğŸ”§ æŠ€è¡“ç´°ç¯€

### BF16 è½‰æ›å¯¦ç¾
```cpp
// å·²å­˜åœ¨æ–¼ ggml-impl.h
static inline ggml_bf16_t ggml_compute_fp32_to_bf16(float s) {
    ggml_bf16_t h;
    union { float f; uint32_t i; } u;
    u.f = s;
    if ((u.i & 0x7fffffff) > 0x7f800000) { /* nan */
        h.bits = (u.i >> 16) | 64; /* force to quiet */
        return h;
    }
    h.bits = (u.i + (0x7fff + ((u.i >> 16) & 1))) >> 16;
    return h;
}

static inline float ggml_compute_bf16_to_fp32(ggml_bf16_t h) {
    union { float f; uint32_t i; } u;
    u.i = (uint32_t)h.bits << 16;
    return u.f;
}
```

### å‘é‡åŒ–å„ªåŒ–
```cpp
// ä½¿ç”¨ AVX512BF16 æŒ‡ä»¤é›†
#if defined(__AVX512BF16__)
void fake_quant_fp32_to_bf16_avx512(const float* src, float* dst, int n) {
    for (int i = 0; i + 16 <= n; i += 16) {
        __m512 fp32_vec = _mm512_loadu_ps(src + i);
        __m256i bf16_vec = _mm512_cvtneps_pbh(fp32_vec);
        __m512 result = _mm512_cvtpbh_ps(bf16_vec);
        _mm512_storeu_ps(dst + i, result);
    }
}
#endif
```

---

## ğŸš§ å·²çŸ¥é™åˆ¶èˆ‡æŒ‘æˆ°

### æŠ€è¡“æŒ‘æˆ°
1. **æ€§èƒ½é–‹éŠ·**: å‹•æ…‹è½‰æ›å¯èƒ½å½±éŸ¿æ¨ç†é€Ÿåº¦
2. **è¨˜æ†¶é«”æ‹·è²**: éœ€è¦é¡å¤–çš„ç·©è¡å€é€²è¡Œè½‰æ›
3. **ç²¾åº¦æ§åˆ¶**: å¦‚ä½•ç²¾ç¢ºæ§åˆ¶é‡åŒ–çš„æ‡‰ç”¨ç¯„åœ

### è§£æ±ºæ–¹æ¡ˆ
1. **æƒ°æ€§é‡åŒ–**: åªåœ¨å¿…è¦æ™‚é€²è¡Œè½‰æ›
2. **In-place æ“ä½œ**: æ¸›å°‘è¨˜æ†¶é«”æ‹·è²
3. **å±¤ç´šæ§åˆ¶**: ç´°ç²’åº¦çš„é‡åŒ–æ§åˆ¶

---

## ğŸ“š ç›¸é—œè³‡æº

### ç¨‹å¼ç¢¼ä½ç½®
- **GGML BF16 å¯¦ç¾**: `ggml/src/ggml-impl.h:436-470`
- **æ•¸æ“šé¡å‹å®šç¾©**: `ggml/include/ggml.h:358-398`
- **è½‰æ›å‡½æ•¸**: `ggml/src/ggml.c:420-470`
- **å‘é‡æ“ä½œ**: `ggml/src/ggml-cpu/vec.h`

### åƒè€ƒæ–‡ç»
1. [Brain Floating Point Format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)
2. [Intel AVX512-BF16 Documentation](https://software.intel.com/content/www/us/en/develop/articles/intel-avx-512-bf16-instructions.html)
3. [GGML Documentation](https://github.com/ggml-org/ggml)

---

## âœ… ç¸½çµ

é€™å€‹ç ”ç©¶ç™¼ç¾ **GGML å·²ç¶“å…·å‚™å®Œæ•´çš„ BF16 åŸºç¤è¨­æ–½**ï¼Œé€™å¤§å¤§ç°¡åŒ–äº†å¯¦ç¾ fake quantization çš„è¤‡é›œåº¦ã€‚ä¸»è¦å·¥ä½œå°‡é›†ä¸­åœ¨ï¼š

1. **æ‡‰ç”¨å±¤æ•´åˆ**: å°‡ç¾æœ‰çš„ BF16 æ”¯æ´æ•´åˆåˆ°æ¨ç†ç®¡é“
2. **ç”¨æˆ¶ä»‹é¢**: æä¾›ç°¡å–®æ˜“ç”¨çš„å‘½ä»¤è¡Œå’Œ API ä»‹é¢  
3. **æ€§èƒ½å„ªåŒ–**: åˆ©ç”¨ç¾æœ‰çš„å‘é‡åŒ–å’Œç¡¬é«”åŠ é€ŸåŠŸèƒ½
4. **æ¸¬è©¦é©—è­‰**: ç¢ºä¿ç²¾åº¦å’Œæ€§èƒ½ç¬¦åˆé æœŸ

ç›¸æ¯”å¾é›¶é–‹å§‹å¯¦ç¾ BF16 æ”¯æ´ï¼Œé€™å€‹æ–¹æ¡ˆæ›´åŠ å¯è¡Œä¸”é¢¨éšªè¼ƒä½ã€‚

---

## ğŸ§ª å¯¦éš›æ¸¬è©¦çµæœèˆ‡ç²¾åº¦åˆ†æ (2025-01-14)

### æ¸¬è©¦ç’°å¢ƒ
- **æ¨¡å‹**: TinyLlama-1.1B-Chat-v1.0 (Q4_K_M é‡åŒ–, 636MB)
- **å¹³å°**: WSL2/Linux (Red Hat 8.5.0) + GCC 8.5.0  
- **æ¸¬è©¦é¡å‹**: BF16, F16, F32 å‡é‡åŒ–

### ğŸ”¬ ç²¾åº¦å½±éŸ¿åˆ†æ

#### 1. æ•¸å€¼ç²¾åº¦å°æ¯”
```cpp
// BF16 ç²¾åº¦ç‰¹æ€§:
// - æŒ‡æ•¸ä½: 8ä½ (èˆ‡ FP32 ç›¸åŒ)
// - å°¾æ•¸ä½: 7ä½ (FP32: 23ä½, FP16: 10ä½) 
// - ç¯„åœ: èˆ‡ FP32 ç›¸åŒ (~1e-38 åˆ° ~3e38)
// - ç²¾åº¦: ç´„ 2-3 ä½åé€²åˆ¶æœ‰æ•ˆæ•¸å­—
```

#### 2. è¼¸å‡ºä¸€è‡´æ€§æ¸¬è©¦çµæœ
**æ¸¬è©¦ç™¼ç¾**: å°æ–¼Q4_K_Mé‡åŒ–æ¨¡å‹ï¼ŒBF16å‡é‡åŒ–æœªç”¢ç”Ÿå¯è§€å¯Ÿçš„æ–‡æœ¬å·®ç•°ã€‚

| é‡åŒ–é¡å‹ | è¼¸å‡ºæ–‡æœ¬ | èˆ‡åŸºæº–å·®ç•° |
|---------|---------|-----------|
| åŸºæº– (ç„¡å‡é‡åŒ–) | "Machine learning is the study of algorithms..." | - |
| BF16 å‡é‡åŒ– | "Machine learning is the study of algorithms..." | **å®Œå…¨ç›¸åŒ** |
| F16 å‡é‡åŒ– | "Machine learning is the study of algorithms..." | **å®Œå…¨ç›¸åŒ** |
| F32 å‡é‡åŒ– | "Machine learning is the study of algorithms..." | **å®Œå…¨ç›¸åŒ** |

#### 3. æ€§èƒ½å½±éŸ¿æ¸¬é‡
| æŒ‡æ¨™ | åŸºæº– | BF16 å‡é‡åŒ– | é–‹éŠ· |
|------|------|------------|------|
| åŠ è¼‰æ™‚é–“ | 219.23ms | 225.84ms | **+3.0%** |
| Prompt è™•ç† | 68.33ms/token | 70.76ms/token | **+3.6%** |
| ç”Ÿæˆé€Ÿåº¦ | 75.41ms/token | 79.13ms/token | **+4.9%** |

### ğŸ“Š ç²¾åº¦æå¤±åˆ†æ

#### ç†è«–ç²¾åº¦æå¤±
```
FP32 â†’ BF16 è½‰æ›æå¤±:
- å°¾æ•¸æˆªæ–·: 23ä½ â†’ 7ä½ (æå¤±16ä½ç²¾åº¦)
- ç†è«–ç²¾åº¦æ¯”: ~2^-16 â‰ˆ 1.5e-5 (ç›¸å°èª¤å·®)
```

#### å¯¦éš›è§€å¯Ÿçµæœ
1. **Q4_K_Mæ¨¡å‹ç·©è¡æ•ˆæ‡‰**: 
   - æ¨¡å‹æœ¬èº«å·²é‡åŒ–åˆ°4ä½ï¼Œé€²ä¸€æ­¥çš„logitsç²¾åº¦æå¤±å½±éŸ¿æœ‰é™
   - BF16çš„8ä½æŒ‡æ•¸ç¯„åœè¶³ä»¥è¦†è“‹logitsæ•¸å€¼ç¯„åœ

2. **æ¡æ¨£æ©Ÿåˆ¶çš„å½±éŸ¿**:
   - Top-kã€Top-p æ¡æ¨£èƒ½å®¹å¿å°å¹…logitsè®ŠåŒ–
   - æº«åº¦åƒæ•¸(0.8)é€²ä¸€æ­¥å¹³æ»‘äº†ç²¾åº¦å·®ç•°

3. **ç´¯ç©èª¤å·®æœ‰é™**:
   - å‡é‡åŒ–åƒ…å½±éŸ¿ç•¶å‰tokené¸æ“‡
   - ä¸å­˜åœ¨èª¤å·®åœ¨åºåˆ—ä¸­çš„ç´¯ç©å‚³æ’­

### ğŸ¯ æ¸¬è©¦çµè«–

#### åŠŸèƒ½é©—è­‰
- âœ… **å¯¦ç¾æ­£ç¢º**: å‡é‡åŒ–æŒ‰è¨­è¨ˆå·¥ä½œï¼ŒæˆåŠŸæ‡‰ç”¨ç²¾åº¦è½‰æ›
- âœ… **é¡å‹æ”¯æŒ**: BF16ã€F16ã€F32 å…¨éƒ¨æ¸¬è©¦é€šé  
- âœ… **ç¸®æ”¾æ§åˆ¶**: 0.5å’Œ1.0ç¸®æ”¾å› å­æ­£å¸¸å·¥ä½œ
- âœ… **æ€§èƒ½å¯æ¥å—**: <5% é–‹éŠ·å±¬æ–¼å¯æ¥å—ç¯„åœ

#### ç²¾åº¦å½±éŸ¿è©•ä¼°
- **ç•¶å‰æ¸¬è©¦å±€é™**: Q4_K_Mæ¨¡å‹å°ç²¾åº¦è®ŠåŒ–ä¸æ•æ„Ÿ
- **BF16å„ªå‹¢ç¢ºèª**: ç¶­æŒFP32æ•¸å€¼ç¯„åœï¼Œé©åˆlogitsè™•ç†
- **å¯¦ç”¨æ€§é©—è­‰**: åŠŸèƒ½å®Œæ•´ï¼Œå¯ç”¨æ–¼æ›´æ•æ„Ÿçš„æ¨¡å‹åˆ†æ

#### å»ºè­°å¾ŒçºŒæ¸¬è©¦
1. **é«˜ç²¾åº¦æ¨¡å‹**: æ¸¬è©¦F16æˆ–F32åŸå§‹ç²¾åº¦æ¨¡å‹
2. **æ•æ„Ÿä»»å‹™**: æ•¸å­¸è¨ˆç®—ã€é‚è¼¯æ¨ç†ç­‰ç²¾åº¦è¦æ±‚é«˜çš„å ´æ™¯
3. **å¤§è¦æ¨¡æ¸¬è©¦**: é•·æ–‡æœ¬ç”Ÿæˆä¸­çš„ç´¯ç©æ•ˆæ‡‰åˆ†æ
4. **æ¶æ§‹å°æ¯”**: ä¸åŒæ¨¡å‹æ¶æ§‹å°ç²¾åº¦è®ŠåŒ–çš„æ•æ„Ÿæ€§

### ğŸ”® æŠ€è¡“æ„ç¾©
é€™æ¬¡å¯¦ç¾è­‰æ˜äº†åœ¨ç¾æœ‰GGMLåŸºç¤è¨­æ–½ä¸Šæ§‹å»ºå‡é‡åŒ–åŠŸèƒ½çš„å¯è¡Œæ€§ï¼Œç‚ºå¾ŒçºŒçš„é‡åŒ–ç ”ç©¶å’Œæ¨¡å‹ç²¾åº¦åˆ†ææä¾›äº†é‡è¦å·¥å…·ã€‚é›–ç„¶åœ¨Q4_K_Mæ¨¡å‹ä¸Šè§€å¯Ÿåˆ°çš„å½±éŸ¿æœ‰é™ï¼Œä½†è©²åŠŸèƒ½ç‚ºæ¢ç´¢ä¸åŒç²¾åº¦å°æ¨¡å‹è¡Œç‚ºçš„å½±éŸ¿é–‹é—¢äº†æ–°çš„å¯èƒ½æ€§ã€‚